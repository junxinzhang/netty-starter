# Netty学习day01：同步阻塞式I/O源码分析

## 服务端TimerServer
`TimerServer`根据传入的参数设置监听端口，如果没有入参，使用默认值8080。
启动后会发现主线程阻塞在ServerSocket的accept()方法上：可通过`JConsole`查看线程堆栈日志可以发现。
（_`JConsole`是一个图形监视工具，用于监视本地或远程计算机上的Java虚拟机（JVM）和Java应用程序。_）[点击查看详情](https://en.wikipedia.org/wiki/JConsole)
    
## 客户端TimerClient
客户端通过PrintWriter向服务端发送"QUERY TIME ORDER"指令，然后通过BufferedReader的readLine读取响应并打印。
    
## 同步阻塞式I/O存在的问题:
每当有一个新的客户端请求接入时，服务端必须创建一个新的线程处理新接入的客户端链路，一个线程只能处理一个客户端连接。
在高性能服务器应用领域，往往需要成千上万个客户端的并发连接，这种模型显然无法满足高性能，高并发的接入场景。
为了改进一个线程对应一个连接的模型，后来又演变出了一种通过线程池或者消息队列实现一个或多个线程处理N个客户端的模型，
由于它的底层通信机制仍然是使用同步阻塞I/O，所以被称为`伪异步`。
    
# Netty学习day02：**`伪异步`**阻塞式I/O源码分析

## 服务端TimerServer改造
`伪异步`I/O的主函数代码发生了变化，我们首先创建一个服务器时间处理类的线程池，但接收到新的客户端连接时，将请求Socket封装成一个task，
然后调用线程池的execute()方法执行，从而避免了每一个请求进入都创建一个新的线程。
由于线程池和消息队列都是有界的，因此，无论客户端并发连接数多大， 它都不会导致线程个数过于膨胀或者内存溢出，相比于一连接一线程模型，是
一种改良。
`伪异步`I/O通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽的问题。但是由于它底层的通信依然采用同步
阻塞模型，因此无法从根本上解决问题。

## `伪异步`I/O存在的问题:
当对Socket的输入流进行读取操作的时候，它会一直阻塞下去，直到发生如下三种事件
- 有数据可读
- 可用数据已经读取完毕
- 发生NPE或者I/O异常  
这意味着当对方发送请求或者应答消息比较缓慢，或者网络传输较慢时，读取输入流一方的通信将被长时间阻塞，如果对方要60s才能够将数据发送完成，
读取一方的I/O线程也将会被阻塞60s，在此期间，其他接入消息只能在消息队列中排队。  
当调用OutputStream的write()方法写输出流的时候，它将会被阻塞，知道所有要发送的字节全部写入完毕，或者发生异常。学习过TCP/IP相关知识
的人都知道，当消息的接收方处理缓慢的时候，将不能及时的从缓冲区中读取数据，这将会导致发送方的`tcp windows size`不断减小，直到为0，双
方处于`Keep-Alive`状态，消息发送方将不能再向TCP缓冲区写入消息，这时如果采用的是同步阻塞I/O，write操作将会被无限期阻塞，直到`tcp 
windows size`大于0或者发生异常。  
通过对输入流和输出流的API文档进行分析，我们了解到读和写操作都是同步阻塞的，阻塞的时间取决于对方I/O线程的处理速度和网络I/O的传输速度。
本质上来讲，我们无法保证生产环境的网络状况和对端的应用程序能足够快，如果我们的应用程序依赖于对方的处理速度，它的可靠性就非常差。也许在实
验室环境的性能测试结果令人满意，但是一旦上线运行，而面对恶劣的网络环境和良莠不齐的第三方系统，问题就会如火山一样喷发。  
`伪异步`I/O实际上仅仅是对之前I/O线程模型的一个简单优化，它无法从根本上解决同步I/O导致的通信线程阻塞问题。
### 通信对方返回应答时间过长会引起的级联故障
- 服务端处理缓慢，返回应答消息耗费60s，平时只需要10ms
- 采用`伪异步`I/O的线程正在读取故障服务节点的响应，由于读取输入流是阻塞的，它将会被同步阻塞60s
- 加入所有的线程都被故障服务器阻塞，那后续所有的I/O消息都将在队列中排队
- 由于线程池采用阻塞队列实现，但队列积满之后，后续入队的操作将被阻塞。
- 由于前端只有一个Accptor线程接收客户端接入，他被阻塞在线程池的同步阻塞队列之后，新的客户端请求消息将被拒绝，客户端将会产生大量的连接超时。
- 由于几乎所有的连接都超时，调用者会认为系统已经崩溃，无法接收新的请求消息。



